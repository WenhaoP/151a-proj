---
title: "STAT151A Final Project, Fall 2021"
author: "Wenhao Pan, Rachel Chen, Richard Shuai"
date: "December 17, 2021"
output:
  pdf_document: 
    toc: true
    number_sections: true
  html_document:
    df_print: paged
urlcolor: blue

---
\newpage

```{r include = FALSE}
knitr::opts_chunk$set(
  echo = FALSE,          # don't show code
  warning = FALSE,       # don't show warnings
  message = FALSE,       # don't show messages (less serious warnings)
  cache = FALSE,         # set to TRUE to save results from last compilation
  fig.align = "center"   # center figures
)

library(dplyr)
library(ggplot2)
library(MASS)
library(car)

```

# Introduction
Baby’s mass is correlated with mortality risk and potential future developmental problems. For example, [researchers in Denmark](https://www.reuters.com/article/us-health-iq-birth-weight/birth-weight-may-impact-intelligence-throughout-life-idUSKCN18E29J) found that babies with birth weights of less than 5 pounds are more likely to experience health complications and even a lower intelligence quotient as children. Thus, it makes sense for healthcare workers and parents to want to predict a baby's weight based on current information. Intuitively speaking, a baby's mass could be predicted by a lot of factors such as the health of the parents, the sex of the baby, the mother’s pregnancy records, etc. In this project, we aim to use linear models to answer the following two questions regarding the baby’s weight.  

1. How does intervening a pregnant woman’s living habits or behaviors affect her baby’s birth weight in the future?  
2. Given the information about an expecting family, what is our best prediction of their baby’s weight?
The first question is more related to causal inference, and its answer could help doctors give suggestions to a pregnant woman for delivering a normally weighted baby. The second one is more related to prediction, and its answer could help doctors conjecture a baby’s weight right before delivery.

# Data Description
```{r}
birth <- read.csv("data/US_births(2018).csv")
```
This dataset was taken from the [National Center for Health Statistics](https://www.cdc.gov/nchs/data_access/vitalstatsonline.htm), and contains information about 3.8 million childbirths in the US in 2018. There are 55 columns, so we grouped them into the following categories:  

- Delivery situation ex) place of birth, number of people around, birth time
- The baby's health information ex) period of gestation, birth weight
- Parents information ex) marital status, education, race
- Parents health records ex) smoking history, age
- Mother’s pregnancy records ex) number of prenatal visits, prior births

The [User Guide](https://ftp.cdc.gov/pub/Health_Statistics/NCHS/Dataset_Documentation/DVS/natality/UserGuide2018-508.pdf) on the website contains the detailed explanations of each column. We will use the baby birth weight column (`DBWT`) as the response variable, and all other variables will be used as explanatory variables. 

# Data Preprocessing
```{r}
# Remove missing values

# remove missing values in the response variable
clean_birth <- subset(birth, DBWT != 9999)

# remove missing values in the features to be considered for adding interactions
clean_birth <- subset(clean_birth, PRECARE != 99 & CIG_0 != 99 & BMI != 99.9 
               & PREVIS != 99 & MRAVE6 != 9 & PAY_REC != 9
               & FRACE6 != 9 & MEDUC != 9 & FEDUC != 9 
               & NO_RISKS != 9)

# remove missing values in the features not to be considered for adding interactions
clean_birth <- subset(clean_birth, ATTEND != 9 & BFACIL != 9 & FAGECOMB != 99 
               & RF_CESAR != "U" & LD_INDL != "U" & MBSTATE_REC != 3
               & M_Ht_In != 99 & NO_INFEC != 9 & NO_MMORB != 9 
               & PRIORLIVE != 99 & PRIORTERM != 99 & RDMETH_REC != 9)

clean_birth <- clean_birth %>% filter(!is.na(DMAR))

# remove missing values in the features for feature engineering
clean_birth <- subset(clean_birth, DLMP_YY != 9999 & DLMP_MM != 99)
clean_birth <- subset(clean_birth, PWgt_R != 999  & WTGAIN != 99)
clean_birth <- subset(clean_birth, ILLB_R != 999)
```

```{r}
nrow(clean_birth)
```

```{r}
# Feature engineering

# estimate pregnancy length
clean_birth$PREG_LEN <- 12*(2018 - clean_birth$DLMP_YY) +
                        (clean_birth$DOB_MM - clean_birth$DLMP_MM)

# categorize and cap pregnancy length
clean_birth$PREG_LEN[clean_birth$PREG_LEN < 8] <- -1
clean_birth$PREG_LEN[clean_birth$PREG_LEN > 10] <- 99
clean_birth$PREG_LEN <- factor(clean_birth$PREG_LEN)
levels(clean_birth$PREG_LEN) <- c("Early", "8", "9", "10", "Late")

# recode PRECARE
clean_birth$PRECARE[clean_birth$PRECARE < 4 & clean_birth$PRECARE > 0] <- 1
clean_birth$PRECARE[clean_birth$PRECARE < 7 & clean_birth$PRECARE > 3] <- 2
clean_birth$PRECARE[ clean_birth$PRECARE > 6] <- 3

# compute percentage weight gain
clean_birth$WTGAIN_PER <- clean_birth$WTGAIN / clean_birth$PWgt_R

# binarize CIG_0
clean_birth$CIG_0 <- ifelse(clean_birth$CIG_0 > 0, TRUE, FALSE)

# binarize PRIORDEAD
clean_birth$PRIORDEAD <- ifelse(clean_birth$PRIORDEAD > 0, TRUE, FALSE)

# binarize PRIORTERM
clean_birth$PRIORTERM <- ifelse(clean_birth$PRIORTERM > 0, TRUE, FALSE)

# binarize PRIORLIVE
clean_birth$PRIORLIVE <- ifelse(clean_birth$PRIORLIVE > 0, TRUE, FALSE)

# compute first time live birth
clean_birth$FIRST_BIRTH <- ifelse(clean_birth$ILLB_R == 888, TRUE, FALSE)
```

```{r}
# Reduce the dimensionality of the dataset

# drop columns where >99% entries are the same
clean_birth <- clean_birth %>% dplyr::select(!c(DOB_YY, IMP_SEX, IP_GON, MAGE_IMPFLG, 
                                   MAR_IMP, MM_AICU, MTRAN))

# drop redundant columns due to feature engineering
clean_birth <- clean_birth %>% dplyr::select(!c(WTGAIN, PWgt_R, DWgt_R, DOB_MM, 
                                   DOB_WK, DOB_TT, DOB_MM, DLMP_YY,
                                   DLMP_MM, PAY, MHISPX, MRACE15,
                                   MRACE31, MRACEIMP, FHISPX, FRACE15,
                                   FRACE31, RF_CESARN, ILOP_R, ILP_R, ILLB_R))
```

```{r, eval=FALSE}
# write.csv(clean_birth, "data/clean_birth.csv", row.names = FALSE)
```

```{r}
# Factorize categorical variables
clean_birth <- clean_birth %>% mutate_if(is.character, as.factor)
clean_birth <- clean_birth %>% mutate_if(is.logical, as.factor)
clean_birth <- clean_birth %>% mutate(ATTEND = factor(ATTEND), BFACIL = factor(BFACIL), 
                                  DMAR = factor(DMAR), FEDUC = factor(FEDUC), 
                                  FRACE6 = factor(FRACE6), MBSTATE_REC = factor(MBSTATE_REC),
                                  MEDUC = factor(MEDUC), MRAVE6 = factor(MRAVE6), 
                                  NO_INFEC = factor(NO_INFEC),NO_MMORB = factor(NO_MMORB), 
                                  NO_RISKS = factor(NO_RISKS), PAY_REC = factor(PAY_REC),
                                  PRECARE = factor(PRECARE), RDMETH_REC = factor(RDMETH_REC),
                                  RESTATUS = factor(RESTATUS))
```


```{r}

# Subsample datasets
set.seed(151)
EDA_size = 3000
Train_size = 100000
Test_size = 100000
EDA_df <- clean_birth %>% slice_sample(n = EDA_size, replace = TRUE)
Train <- clean_birth %>% slice_sample(n = Train_size, replace = TRUE)
Test <- clean_birth %>% slice_sample(n = Test_size, replace = TRUE)
```

We first propose that due to the excessive size of the original dataset, 3.8 million observations, we plan to randomly subsample three subsets, one with 5000 observations and two with 100000 observations, with replacement as datasets for EDA, training, and testing. This plan balances the computational cost of the analysis and the complexity of our dataset well. We conduct this subsampling plan at the end of data preprocessing.

The priority of our data preprocessing is to reduce the dimensionality of our dataset by filtering out unuseful features. We have 54 explanatory variables, but it is not efficient to analyze each of them evenly. We suspect that some variables can be combined and condensed into a new variable. To systematically filter the features, we split the explanatory variables into five exclusive categories:
Homogeneous: Variables of which more than 99% of entries have the same values.
Minor: We do not consider interaction terms involving these variables.
Major: We do consider interaction terms involving these variables.
Obsolete: After we create a new variable based on these variables, they essentially do not provide enough extra information to be kept. The new variable belongs to “Major”.
Redundant: After we select one from a group of variables including similar information, the rest become redundant or unnecessary to be kept. The selected variable belongs to “Major”.
See the appendix for the names of the variables in each category.

Next, we drop all the observations including any missing value in the columns of “Minor”, “Major”, and “Obsolete” categories. After dropping, we still have about 2.8 million observations left, which are sufficient for subsampling. The missing values in our dataset are not left blank or NA. Instead, they are recoded into values such as '9' or '999', depending on the variable. Thus, we manually look up the recodings from the User Guide and drop the missing values for each feature. Imputing those missing values might be a better approach since if the missing values exemplify a systematic pattern, we might introduce bias by removing all the missing values. However, given the already complicated structure of our dataset, we choose the simpler approach-dropping missing values, noting that the imputation approach is still valuable to be explored. 

Cleaning up missing values allows us to conduct feature engineering, which aims to use domain knowledge to simplify the dataset while maintaining the original information. For example, we create the feature `PREG_LEN` which estimates the pregnancy length by computing the number of months between the last normal mense and delivery date. Then, we categorize `PREG_LEN` into `Early`, `8`, `9`,`10`,`Late` by common sense. Thus, `PREG_LEN` becomes a “Major” variable, and variables about the last normal menses and delivery dates become “Obsolete” variables. `FRACE6`, `FRACE15`, `FRACE31`, and `FHISPX` all describe a father’s race but with different granularity levels. We select `FRACE6` to solely describe a father’s race since we think six racial categories already sufficiently differentiate people. Thus, `FRACE6` is a “Major” variable, and other father’s race variables become “Redundant”. See the code appendix for the complete work of feature engineering. 

Finally, we drop “Homogeneous”, “Obsolete”, and “Redundant” variables. It is obvious that we should drop “Obsolete” and “Redundant” variables to mitigate collinearity and duplication in our dataset. We drop the “Homogeneous” variables because it is highly likely that they essentially have one unique value or a negligible number of other values in subsample datasets. 
We warn that the entire preprocessing approach is considerably subjective, and the following regression analysis is highly dependent on our approach. It may sound like a compromise to the complexity of our dataset to some audiences. We encourage different preprocessing approaches, but we continue with ours since we think it is still reasonable. 

# Exploratory Data Analysis
```{r}
# EDA

# response variable
ggplot(EDA_df, aes(x = DBWT)) +
  geom_histogram()

# Measure of symmetry
DBWT_sym = (quantile(EDA_df$DBWT, 0.75) - median(EDA_df$DBWT)) /
  (median(EDA_df$DBWT) - quantile(EDA_df$DBWT, 0.25))

ggplot(EDA_df, aes(x = WTGAIN_PER, y = DBWT)) +
  geom_point() +
  geom_smooth()

ggplot(EDA_df, aes(x = CIG_0, y = DBWT)) +
  geom_boxplot()

ggplot(EDA_df, aes(x = PRECARE, y = DBWT)) +
  geom_boxplot()

# Binarize PRECARE
EDA_df$PRECARE <- ifelse(EDA_df$PRECARE != 0, TRUE, FALSE)
Train$PRECARE <- ifelse(Train$PRECARE != 0, TRUE, FALSE)
Test$PRECARE <- ifelse(Test$PRECARE != 0, TRUE, FALSE)

# DBWT vs. Binarized PRECARE
ggplot(EDA_df, aes(x = PRECARE, y = DBWT)) +
  geom_boxplot()

# Interactions
ggplot(EDA_df, aes(x = BMI, y = DBWT)) +
  geom_point(aes(colour = PRECARE), alpha = 0.5) +
  geom_smooth(aes(colour = PRECARE))

ggplot(EDA_df, aes(x = PRECARE, y = DBWT)) +
  geom_boxplot(aes(fill = SEX))
```

To gain a better understanding of the variables and data, we performed EDA on our dataset. First, we plotted the distribution of the response variable to verify that our linear regression assumptions are satisfied. From Figure X, we can see that the distribution is quite symmetric and normal-looking (reference QQ plot?). This is desired because symmetric data makes it easier to model, and also normality is an assumption of statistical inference. 

To verify symmetry, we use the formula Upper quartile - Median / Median - Lower Quartile (put this in latex):
			(insert Wenhao’s code for measure of symmetry)
Since the ratio is close to 1, we can safely conclude that our response variable is symmetric.

We next examined the bivariate relationships between our response variable and each explanatory variable in our dataset. In Figure X, we see that as the percentage of weight gained due to pregnancy increases, the birth weight tends to increase. The positive linear relationship with the birth weight visualizes this correlation. 

We also conclude that CIG_0 is an important explanatory variable we want to include in our model, since from the box plot in Figure X we see that the 1st quartile, median, and 3rd quartile of birth weight are all lower if the mother smokes, than if she does not. When plotting birth weight against the prenatal care status of the mother, we find that the distributions of birth weight seem to be most different between mothers who didn’t receive prenatal care and mothers who did. This suggests that the most important difference can be observed when we binarize the explanatory variable for the mother’s prenatal care status. From Figure X, we see that this holds, and we therefore binarize the mother’s prenatal care status for downstream analysis.

In addition to the main effect terms, interaction terms are visualized using boxplots and scatterplots. The box plot in Figure X shows the interaction between SEX and PRECARE. We notice that the difference in the median birth weight for mothers who received prenatal care and who did not changes across the sex of the baby, which motivates using an interaction term. More specifically, such difference is larger in male babies than female babies.

In Figure X, we observe a possible interaction between the mother’s prenatal care status and her BMI when predicting the baby’s birth weight. We see that if a pregnant woman does not receive prenatal care, as her BMI increases, she is more likely to have lighter babies, which could be a signal for an unhealthy baby. This might make sense because obesity is linked to health problems, which consequently affect the health and birth weight of the baby. This, however, may be corrected if the mother received care and possibly took actions to mitigate her health problems, leading to more normal birth weights.

# Model Selection
```{r, eval=FALSE}
# Model Selection

biggest.model <- lm(DBWT ~ ., data = Train)
# summary(biggest.model)

# Remove the columns causing singularity
Train <- Train %>% dplyr::select(!c(RF_CESAR))
biggest.model <- lm(DBWT ~ ., data = Train)
min.model <- lm(DBWT ~ 1, data = Train)
# summary(biggest.model)
# Forward selection with BIC
forward.BIC = step(min.model, direction="forward", scope = formula(biggest.model),
                   k = log(nrow(Train)), trace = 0)

# Backward selection with BIC
backward.BIC = step(biggest.model, direction="backward", 
                 k = log(nrow(Train)), trace = 0)

# Forward selection with AIC
forward.AIC = step(min.model, direction="forward", scope = formula(biggest.model),
                 k = 2, trace = 0)

# Backward selection with AIC
backward.AIC = step(biggest.model, direction="backward", 
                 k = 2, trace = 0)
```

```{r, eval=FALSE}
# Compute the leave-one-out cross-validation errors
for_AIC.cv = mean((residuals(forward.AIC) / (1 - hatvalues(forward.AIC))) ^ 2)
back_AIC.cv = mean((residuals(backward.AIC) / (1 - hatvalues(backward.AIC))) ^ 2)
for_BIC.cv = mean((residuals(forward.BIC) / (1 - hatvalues(forward.BIC))) ^ 2)
back_BIC.cv = mean((residuals(backward.BIC) / (1 - hatvalues(backward.BIC))) ^ 2)
which.min(c(for_AIC.cv, back_AIC.cv, for_BIC.cv, back_BIC.cv))
```


```{r, eval=FALSE}
# Add interaction terms by F-test
full.lm <- lm(DBWT ~ PREG_LEN + M_Ht_In + MRAVE6 + SEX + BMI + WTGAIN_PER + 
    PRIORLIVE + CIG_0 + NO_RISKS + RDMETH_REC + PREVIS + ATTEND + 
    MBSTATE_REC + FRACE6 + PAY_REC + LD_INDL + FEDUC + NO_MMORB + 
    BFACIL + FAGECOMB + NO_INFEC + RESTATUS + MEDUC + PRECARE + 
    DMAR + BMI * PRECARE + WTGAIN_PER * PRECARE + PRECARE * MEDUC +
    PREVIS * PREG_LEN + PREG_LEN * MEDUC + PRECARE * CIG_0 + CIG_0 * SEX +
    PRECARE * PREG_LEN + CIG_0 * PREG_LEN, data = Train)

# Type II Anova
Anova(full.lm)
```

```{r}
# Final model
final.lm <- lm(DBWT ~ PREG_LEN + M_Ht_In + MRAVE6 + SEX + BMI + WTGAIN_PER + 
    PRIORLIVE + CIG_0 + NO_RISKS + RDMETH_REC + PREVIS + ATTEND + 
    MBSTATE_REC + FRACE6 + PAY_REC + LD_INDL + FEDUC + NO_MMORB + 
    BFACIL + FAGECOMB + NO_INFEC + RESTATUS + MEDUC + PRECARE + 
    DMAR + PREVIS * PREG_LEN + PREG_LEN * MEDUC + CIG_0 * PRECARE +
    PRECARE * PREG_LEN + CIG_0 * PREG_LEN, data = Train)
```
We first fit the fullest model with only the main effect terms, which contains 30 explanatory variables or 68 regressors. Such a complicated model with so many regressors is not desired for prediction or causal inference because it will tend to overfit on the training data and therefore generalize poorly on new datasets, even if drawn from the same distribution. Also, such a model would require us to collect lots of information to predict, which will limit the usability of the model in a realistic setting. Moreover, such a model is hard to interpret for causal inference. Thus, we conduct model selection to select a simpler model.

We first remove explanatory variables introducing singularity, which have fitted coefficients `NA`. Next, we construct four models using four different approaches: forward/backward selection with AIC/BIC by `step` function and then select the one with the lowest leave-one-out cross-validation (LOOCV) error. Both AIC and BIC measure the in-sample fitness of a model, while BIC penalizes the model size more when the sample size is large. Lower AIC or BIC in value means a better model. We chose forward and backward selection instead of all subset selection because of the large number of explanatory variables, which makes all subset selection computationally infeasible. We chose the `step` function because it adds or drops categorical variables only as an entire unit instead of splitting them up into unconnected dummy regressors. Ideally, we hope that these four models using different criteria and search strategies would explore diverse model choices, so the final one selected by LOOCV error is the most descriptive.

It turns out that both the models returned by forward and backward selection with AIC have the lowest LOOCV error and are identical in terms of the set of included explanatory variables, so both of them are the best model. We then add the selected interaction terms based on our findings from the EDA to the best model and use the incremental F-test with `Anova` to filter out the insignificant interaction terms. See the code appendix for more details about the entire process. The final model that will be used for both causal inference and prediction is
```{r, echo = TRUE}
formula(final.lm)
```

# Model Diagnostics
Although we require a statistically significant linear model from model selection, the linear model made strong and specific assumptions about the structure of our data (Fox, p266). These assumptions-linearity, constant variance, independent noise, and normality-do not often hold in applications. Moreover, the method of least squares can be very sensitive to unusual or influential data points (Fox, p266).  Thus, to examine the credibility and validity of our model, we use a series of model diagnostics techniques to check the model assumptions and identify unusual or influential data points.

## Linear Modeling Assumptions
```{r}
# Model diagnostic
plot(final.lm, col = rgb(red = 0, green = 0, blue = 0, alpha = 0.2))
# Potential outliers: 76190, 94287, 45730

# compute studentized residuals
stu_res <- studres(final.lm)
Train <- cbind(Train, stu_res)
stu_res_dec <- stu_res[order(abs(stu_res), decreasing = TRUE)]

# Check linearity and constant variance
ggplot(Train, aes(x = BMI, y = stu_res)) +
    geom_point(alpha = 0.1) +
    geom_smooth()
```
First, we will verify our modeling assumptions using various diagnostic plots. We skip the independent noise assumption because we are not dealing with geospatial and time series data, so we could safely assume that noises are independent of each other. 

When plotting the residuals against the fitted values (Figure X), we do not see a clear trend in the spread of residuals as a function of fitted values, which supports our constant variance assumption. Additionally, because the residuals do not show any clear non-linear pattern, the plot (Figure X) supports our linearity assumption. To help us verify normality assumptions, we also plot a quantile-comparison plot of the standardized residuals against the normal distribution (Figure X). Examining the shape of the Q-Q plot, we see that the distribution of the residuals has slightly heavy tails, indicating potential violation of this assumption. Although we can use case bootstrapping to alleviate this issue, we choose to continue with our original data since the issue is not severe. In the scale-location plot, the red line is roughly horizontal, providing additional evidence for the validity of the constant variance assumption (Figure X). 

Finally, we plot the studentized residuals versus one of the explanatory variables, BMI, and look for any patterns (Figure X). The studentized residuals appear to be mostly centered around 0 with no clear pattern, which supports our linearity assumption. However, we notice some downward curvature towards the extreme values of BMI, indicating a possible slight violation of our linearity assumption. Additionally, the spread of the studentized residuals does not seem to have a strong dependence on BMI, thus demonstrating homoscedasticity and indicating support for our constant variance assumption. 

## Unusual, Influential Data Points
```{r}
# Outliers

# test the largest studentized residual
alpha = 0.5
p_value <- pt(stu_res_dec[1], df = final.lm$df.residual - 1, lower.tail = FALSE)
p_value < alpha / nrow(Train) # Bonferroni Correction

# check observations with top 5 largest studentized residuals
Train[head(names(stu_res_dec)),]
```
```{r}
# Influential Points
Train[c(16202, 61480, 49132),]
```
Next, we detect and analyze unusual data points that may significantly affect the fitted coefficients of our model. In our diagnostic plots, we observed a few unusual data points, possibly outliers, with indices 76190, 94287, and 45730 in Figure X. To identify the influential data points, we check if there is any point outside the contour of the Cook’s distance equal to 0.5  in the residuals vs. leverage plot. A larger Cook’s distance means a larger influence of a data point on the coefficient estimation. As we cannot even see the contour in the plot, we claim that no data point is highly influential. which shows how the residuals behave as leverage increases, and we see that the spread does not change with leverage. 

Mention OVERPLOTTING.

# Model Interpretation
Our final model, obtained from forward selection with AIC, contains 103 regressors. For model interpretation, to avoid the issue of post-selection inference, we re-fit the model to the test set. This model will be used for interpreting the model’s coefficients. 

## Causal Inference
```{r}
# Model Interpretation (Causual Inference)
final_test.lm <- lm(DBWT ~ PREG_LEN + M_Ht_In + MRAVE6 + SEX + BMI + WTGAIN_PER + 
    PRIORLIVE + CIG_0 + NO_RISKS + RDMETH_REC + PREVIS + ATTEND + 
    MBSTATE_REC + FRACE6 + PAY_REC + LD_INDL + FEDUC + NO_MMORB + 
    BFACIL + FAGECOMB + NO_INFEC + RESTATUS + MEDUC + PRECARE + 
    DMAR + PREVIS * PREG_LEN + PREG_LEN * MEDUC + CIG_0 * PRECARE +
    PRECARE * PREG_LEN + CIG_0 * PREG_LEN, data = Test)

summary(final_test.lm)
```

Based on the model’s coefficient on `BMI`, we would expect that on average, with all other regressors held constant, a unit increase in a mother’s pre-pregnancy BMI will be associated with an increase of baby birth weight by 17.6692 grams. The interpretation of other coefficients is complicated by interaction terms involving that coefficient. For example, because we included an interaction between `PREG_LEN` and `PRECARE`, we interpret the coefficient on `PRECARETRUE` relative to the reference dummy variable for pregnancy length. The coefficient on `PRECARETRUE` tell us that for mothers with a short estimated pregnancy length (less than 8 months), we expect that undergoing prenatal care will be associated with an average decrease in birth weight by 289.1129 grams when holding all other regressors constant. However, interestingly, the model coefficient on `PREG_LENLate:PRECARETRUE` is significant with a value of 733.0657. This means that for mothers with a late estimated pregnancy length (greater than 10 months), we expect that undergoing prenatal care will be associated with an average *increase* in birth weight by (733.0657   -289.1129) = 443.9528 grams. 

To interpret the relative significance of explanatory variables in the regression, we could compute the standardized coefficients for all numerical variables and compare the corresponding magnitudes. However, we cannot compare the relative importance of categorical variables or interaction terms using this method. For these coefficients, we can rely on the significance of the coefficients as determined by incremental F-tests. Richard: What else should we add here? I think it’s worth computing the actual standardized coeffs and having 1-2 sentences explaining some relative significance. I would be happy to do this. Rachel: should we also add confidence intervals for coefficients?

## Prediction
```{r}
# Model Prediction
MSE.train <- mean(final.lm$residuals ^ 2)
pred.test <- predict(final.lm, Test)
MSE.test <- mean((pred.test - Test$DBWT) ^ 2)
MSE.train
MSE.test

true.test <- Test$DBWT
test.pred.df <- data.frame(cbind(true.test, pred.test))
ggplot(test.pred.df, aes(x = true.test, y = pred.test)) +
  geom_point()

# prediction intervals of five points
five_samples <- Test %>% sample_n(5, replace = TRUE)
predict(final_test.lm, five_samples, interval = "prediction")
```
This model has an adjusted R^2 of 0.3262, which is very close to the R^2 achieved on the [leading board on Kaggle](https://www.kaggle.com/albertum/us-births-2018-linear-regression-r2-0-378). Our model achieves a test set mean squared error (MSE) of 223183.1, compared with a train set MSE of 222923.6. The relatively small difference between the train and test MSEs indicate that our model is not overfitting to the training set, since the model generalizes fairly well to the test set. To further evaluate whether the model will predict future baby birth weights with high precision, we also examine the 95% prediction intervals of a few randomly selected data points from the test set. 
Looking at the distribution of baby birth weights in Figure X, we see that the prediction intervals tend to be very wide relative to the entire distribution of values of `DBWT`, indicating that the model’s predictions are imprecise. Therefore, the model will likely predict future baby birth weights with low precision.

# Discussion
The primary purpose of this project was to determine whether we can predict the birth weight of a baby given information about the expecting family, and which factors we can intervene with to change the baby’s birth weight. Therefore, we must consider the extent to which our linear model can be used for prediction and for causal inference.

As shown above, although our final model does not show signs of overfitting, the prediction intervals on the test set indicate that the model’s predictions of birth weight for new data points are imprecise. This indicates that the model is unsuited for reliable prediction for new data points. Additionally, because the dataset used in this report is specific to US births, it is uncertain how well the model will generalize when predicting the birth weights in other regions of the world. Furthermore, because the test dataset uses only births in 2018, the model’s performance has not been evaluated for predicting birth weights for babies born in the current year. 

It is difficult to draw definitive applications for causal inference, because, according to [a study](https://www.sciencedaily.com/releases/2019/05/190501114600.htm) from the University of Exeter, a baby’s weight is mostly determined by his or her genetic code. This suggests that the explanatory variables in our dataset act primarily as proxies for the true cause of a baby’s birth weight. Intervening with the explanatory variables in this dataset therefore does not guarantee that the baby’s birth weight will change. Furthermore, only certain explanatory variables included within the model are intervenable. For example, it would not make sense for us to recommend expecting parents to change their race or change the sex of the baby, but we can recommend them to stop smoking cigarettes, schedule more prenatal visits, or adjust their BMI by following a weight management program. Thus, we suggest caution when attempting to use this model for causal inference. 

# Conclusion
In this report, we explored data about child births in the United States in 2018 from the National Center for Health Statistics. Given a set of explanatory variables describing information about the baby’s parents, such as education status, BMI, and smoking history, we constructed linear models to predict a baby’s birth weight. We selected main effects for our model using forward selection with AIC and added interaction terms based on EDA and incremental F-tests. Our final model included 103 regressors, achieving an adjusted R^2 of 0.3262. Our analysis demonstrated shortcomings in terms of precision when predicting the response variable birth weight. 

For further analysis, we might explore imputation to minimize bias in our dataset. For numerical variables, we could replace the missing values by the mean. For categorical variables, we could replace the missing values by the mode. 

We may also leverage more sophisticated machine learning methods such as random forest, KNN, or deep learning. 
